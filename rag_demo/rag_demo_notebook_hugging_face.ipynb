{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "content-table",
      "metadata": {},
      "source": [
        "# Table of Contents\n",
        "1. [Introduction](#introduction)\n",
        "2. [Load Environment Variables](#load-environment-variables)\n",
        "3. [Set up LLM and Embedding Model](#set-up-llm-and-embedding-model)\n",
        "4. [Create In-memory Vector Store](#create-in-memory-vector-store)\n",
        "5. [Create and Load On-disk Vector Store](#create-and-load-on-disk-vector-store)\n",
        "6. [Update and Delete Data](#update-and-delete-data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "introduction",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "In this basic RAG (Retrieval-Augmented Generation) example, we take a Paul Graham essay, split it into chunks, embed it using an open-source embedding model, load it into Chroma, and then query it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-environment-variables",
      "metadata": {},
      "source": [
        "## Load Environment Variables\n",
        "\n",
        "In this section, we will load the necessary environment variables required for accessing the Hugging Face LLMs and Embedding models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "import-libraries",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Required Libraries\n",
        "from dotenv import load_dotenv\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "load-env-vars",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Environment Variables\n",
        "load_dotenv()\n",
        "\n",
        "# Access the Hugging Face environment variables\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "set-up-llm-and-embedding-model",
      "metadata": {},
      "source": [
        "## Set up LLM and Embedding Model\n",
        "\n",
        "In this section, we will set up the Hugging Face LLM and embedding model using the loaded environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "setup-llm-embedding",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/rag-demo-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "Settings.llm_model = HuggingFaceInferenceAPI(model_name=\"HuggingFaceH4/zephyr-7b-alpha\", token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "93c70ba4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CompletionResponse(text=\" beyond!\\n\\nThe Toy Story franchise has been a beloved part of pop culture for over two decades, and it's not slowing down anytime soon. The latest installment, Toy Story 4, is set to hit theaters this summer, and it's already generating buzz.\\n\\nThe movie follows the adventures of Woody, Buzz, and the gang as they embark on a new adventure with a new toy, Forky. The trailer for the movie has been released, and it's already getting fans excited for the film.\\n\\nOne of the most exciting things about Toy Story 4 is the return of some beloved characters. Bo Peep, who was last seen in Toy Story 2, is back and looking better than ever. She's now a modern, independent woman, and her new look has been getting a lot of attention.\\n\\nAnother exciting addition to the movie is the introduction of new characters, including Forky, who is voiced by Tony Hale. Forky is a spork with a popsicle stick for a handle, and he's not exactly thrilled about being a toy.\\n\\nThe trailer for Toy Story 4 has been viewed over 10 million\", additional_kwargs={}, raw=None, logprobs=None, delta=None)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Settings.llm_model.complete(\"To infinity, and\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-documents-and-create-vector-store",
      "metadata": {},
      "source": [
        "## Create In-memory Vector Store\n",
        "\n",
        "In this section, we will load the documents, create a Chroma vector store, and store the embedded documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "import-chroma",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Chroma and other required libraries\n",
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "create-vector-store",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Chroma client and a new collection\n",
        "chroma_client = chromadb.EphemeralClient()\n",
        "chroma_collection = chroma_client.create_collection(\"quickstart\")\n",
        "\n",
        "# Load documents\n",
        "documents = SimpleDirectoryReader(\"../data/paul_graham/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "setup-vector-store",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "<b>\n",
              "\n",
              "The author worked on writing and programming before college. He wrote short stories, but they had hardly any plot, just characters with strong feelings. He also tried writing programs on punch cards for an IBM 1401 in 9th grade, but he couldn't figure out what to do with it. However, with microcomputers, everything changed, and he started programming in earnest. He wrote simple games, a program to predict how high his model rockets would fly, and a word processor that his father used to write at least one book.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set up ChromaVectorStore and load in data\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context, embed_model=Settings.embed_model\n",
        ")\n",
        "\n",
        "# Query Data\n",
        "query_engine = index.as_query_engine(llm=Settings.llm_model)\n",
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save-and-load-from-disk",
      "metadata": {},
      "source": [
        "## Create and Load On-disk Vector Store\n",
        "\n",
        "Extending the previous example, if you want to save to disk, simply initialize the Chroma client and pass the directory where you want the data to be saved to.\n",
        "\n",
        "`Caution`: Chroma makes a best-effort to automatically save data to disk, however multiple in-memory clients can stomp each other's work. As a best practice, only have one client per path running at any given time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "save-load-disk",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "<b>\n",
              "\n",
              "The author worked on writing and programming before college. He wrote short stories, but they had hardly any plot, just characters with strong feelings. He also tried writing programs on punch cards for an IBM 1401 in 9th grade, but he couldn't figure out what to do with it. However, with microcomputers, everything changed, and he started programming in earnest. He wrote simple games, a program to predict how high his model rockets would fly, and a word processor that his father used to write at least one book.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Save to disk\n",
        "\n",
        "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context, embed_model=Settings.embed_model\n",
        ")\n",
        "\n",
        "# Load from disk\n",
        "db2 = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "chroma_collection = db2.get_or_create_collection(\"quickstart\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store,\n",
        "    embed_model=Settings.embed_model,\n",
        ")\n",
        "\n",
        "# Query Data from the persisted index\n",
        "query_engine = index.as_query_engine(llm=Settings.llm_model)\n",
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "update-and-delete-data",
      "metadata": {},
      "source": [
        "## Update and Delete Data\n",
        "\n",
        "While building toward a real application, you want to go beyond adding data, and also update and delete data.\n",
        "\n",
        "Chroma has users provide `ids` to simplify the bookkeeping here. `ids` can be the name of the file, or a combined hash like `filename_paragraphNumber`, etc.\n",
        "\n",
        "Here is a basic example showing how to do various operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d9411826",
      "metadata": {
        "id": "d9411826",
        "outputId": "d73c2ac0-73a6-40b2-8b27-b32124410d2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'_node_content': '{\"id_\": \"4605d813-546c-4d77-b76d-8dbd0f3c5bfd\", \"embedding\": null, \"metadata\": {\"file_path\": \"/Users/Yuxuan_Zhang/Downloads/demo/rag-demo/rag_demo/../data/paul_graham/paul_graham_essay.txt\", \"file_name\": \"paul_graham_essay.txt\", \"file_type\": \"text/plain\", \"file_size\": 75042, \"creation_date\": \"2025-02-09\", \"last_modified_date\": \"2025-02-09\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"90f25e82-2dbd-4d0f-b3f4-02486fa43893\", \"node_type\": \"4\", \"metadata\": {\"file_path\": \"/Users/Yuxuan_Zhang/Downloads/demo/rag-demo/rag_demo/../data/paul_graham/paul_graham_essay.txt\", \"file_name\": \"paul_graham_essay.txt\", \"file_type\": \"text/plain\", \"file_size\": 75042, \"creation_date\": \"2025-02-09\", \"last_modified_date\": \"2025-02-09\"}, \"hash\": \"0c3c3f46cac874b495d944dfc4b920f6b68817dbbb1699ecc955d1fafb2bf87b\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"ce221511-e825-4313-9146-75f0b6882dd1\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"a86dcfe59f6bd34948c27f71ce3559236525c77d68fd0be377e2548e308f9ec0\", \"class_name\": \"RelatedNodeInfo\"}}, \"metadata_template\": \"{key}: {value}\", \"metadata_separator\": \"\\\\n\", \"text\": \"\", \"mimetype\": \"text/plain\", \"start_char_idx\": 0, \"end_char_idx\": 4170, \"metadata_seperator\": \"\\\\n\", \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"class_name\": \"TextNode\"}', '_node_type': 'TextNode', 'author': 'Paul Graham', 'creation_date': '2025-02-09', 'doc_id': '90f25e82-2dbd-4d0f-b3f4-02486fa43893', 'document_id': '90f25e82-2dbd-4d0f-b3f4-02486fa43893', 'file_name': 'paul_graham_essay.txt', 'file_path': '/Users/Yuxuan_Zhang/Downloads/demo/rag-demo/rag_demo/../data/paul_graham/paul_graham_essay.txt', 'file_size': 75042, 'file_type': 'text/plain', 'last_modified_date': '2025-02-09', 'ref_doc_id': '90f25e82-2dbd-4d0f-b3f4-02486fa43893'}\n",
            "count before 22\n",
            "count after 21\n"
          ]
        }
      ],
      "source": [
        "doc_to_update = chroma_collection.get(limit=1)\n",
        "doc_to_update[\"metadatas\"][0] = {\n",
        "    **doc_to_update[\"metadatas\"][0],\n",
        "    **{\"author\": \"Paul Graham\"},\n",
        "}\n",
        "chroma_collection.update(\n",
        "    ids=[doc_to_update[\"ids\"][0]], metadatas=[doc_to_update[\"metadatas\"][0]]\n",
        ")\n",
        "updated_doc = chroma_collection.get(limit=1)\n",
        "print(updated_doc[\"metadatas\"][0])\n",
        "\n",
        "# delete the last document\n",
        "print(\"count before\", chroma_collection.count())\n",
        "chroma_collection.delete(ids=[doc_to_update[\"ids\"][0]])\n",
        "print(\"count after\", chroma_collection.count())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rag-demo-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
