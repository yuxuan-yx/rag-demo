{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "content-table",
      "metadata": {},
      "source": [
        "# Table of Contents\n",
        "1. [Introduction](#introduction)\n",
        "2. [Load Environment Variables](#load-environment-variables)\n",
        "3. [Set up LLM and Embedding Model](#set-up-llm-and-embedding-model)\n",
        "4. [Create In-memory Vector Store](#create-in-memory-vector-store)\n",
        "5. [Create and Load On-disk Vector Store](#create-and-load-on-disk-vector-store)\n",
        "6. [Update and Delete Data](#update-and-delete-data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "introduction",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "In this basic RAG (Retrieval-Augmented Generation) example, we take a Paul Graham essay, split it into chunks, embed it using an open-source embedding model, load it into Chroma, and then query it.\n",
        "\n",
        "LlamaIndex is a framework for building Agentic Applications and Context-Augmented LLM Applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-environment-variables",
      "metadata": {},
      "source": [
        "## Load Environment Variables\n",
        "\n",
        "In this section, we will load the necessary environment variables required for accessing the OpenAI API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "import-libraries",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Required Libraries\n",
        "from dotenv import load_dotenv\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "load-env-vars",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Environment Variables\n",
        "load_dotenv()\n",
        "\n",
        "# Access the OpenAI environment variables\n",
        "openai_base_url = os.getenv('OPENAI_BASE_URL')\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "set-up-llm-and-embedding-model",
      "metadata": {},
      "source": [
        "## Set up LLM and Embedding Model\n",
        "\n",
        "In this section, we will set up the OpenAI language model and embedding model using the loaded environment variables. <br>\n",
        "\n",
        "The Settings is a bundle of commonly used resources used during the indexing and querying stage in a LlamaIndex workflow/application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "setup-llm-embedding",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up OpenAI\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", api_key=openai_api_key, api_base=openai_base_url)\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", api_key=openai_api_key, api_base=openai_base_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-documents-and-create-vector-store",
      "metadata": {},
      "source": [
        "## Create In-memory Vector Store\n",
        "\n",
        "In this section, we will load the documents, create a Chroma vector store, and store the embedded documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "import-chroma",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Chroma and other required libraries\n",
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e56f9f7e",
      "metadata": {},
      "source": [
        "### SimpleDirectory Reader supports:\n",
        "By default SimpleDirectoryReader will try to read any files it finds, treating them all as text. In addition to plain text, it explicitly supports the following file types, which are automatically detected based on file extension:\n",
        "\n",
        "- .csv - comma-separated values\n",
        "- .docx - Microsoft Word\n",
        "- .epub - EPUB ebook format\n",
        "- .hwp - Hangul Word Processor\n",
        "- .ipynb - Jupyter Notebook\n",
        "- .jpeg, .jpg - JPEG image\n",
        "- .mbox - MBOX email archive\n",
        "- .md - Markdown\n",
        "- .mp3, .mp4 - audio and video\n",
        "- .pdf - Portable Document Format\n",
        "- .png - Portable Network Graphics\n",
        "- .ppt, .pptm, .pptx - Microsoft PowerPoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "create-vector-store",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Chroma client and a new collection\n",
        "chroma_client = chromadb.EphemeralClient()\n",
        "chroma_collection = chroma_client.create_collection(\"quickstart\")\n",
        "\n",
        "# Load documents\n",
        "documents = SimpleDirectoryReader(\"../data/paul_graham/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "setup-vector-store",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "<b>The author worked on writing and programming before college.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set up ChromaVectorStore and load in data\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context, embed_model=Settings.embed_model\n",
        ")\n",
        "\n",
        "# Query Data\n",
        "query_engine = index.as_query_engine(llm=Settings.llm)\n",
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save-and-load-from-disk",
      "metadata": {},
      "source": [
        "## Create and Load On-disk Vector Store\n",
        "\n",
        "Extending the previous example, if you want to save to disk, simply initialize the Chroma client and pass the directory where you want the data to be saved to.\n",
        "\n",
        "`Caution`: Chroma makes a best-effort to automatically save data to disk, however multiple in-memory clients can stomp each other's work. As a best practice, only have one client per path running at any given time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "save-load-disk",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "<b>The author worked on writing short stories and programming, particularly on an IBM 1401 computer in 9th grade using an early version of Fortran. Later on, the author transitioned to working on microcomputers, starting with a TRS-80 in about 1980, where they wrote simple games, programs, and a word processor.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Save to disk\n",
        "\n",
        "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context, embed_model=Settings.embed_model\n",
        ")\n",
        "\n",
        "# Load from disk\n",
        "db2 = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "chroma_collection = db2.get_or_create_collection(\"quickstart\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store,\n",
        "    embed_model=Settings.embed_model,\n",
        ")\n",
        "\n",
        "# Query Data from the persisted index\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "update-and-delete-data",
      "metadata": {},
      "source": [
        "## Update and Delete Data\n",
        "\n",
        "While building toward a real application, you want to go beyond adding data, and also update and delete data.\n",
        "\n",
        "Chroma has users provide `ids` to simplify the bookkeeping here. `ids` can be the name of the file, or a combined hash like `filename_paragraphNumber`, etc.\n",
        "\n",
        "Here is a basic example showing how to do various operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d9411826",
      "metadata": {
        "id": "d9411826",
        "outputId": "d73c2ac0-73a6-40b2-8b27-b32124410d2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'_node_content': '{\"id_\": \"8b198df5-6c1a-49c3-9f37-9dfa74e18ac0\", \"embedding\": null, \"metadata\": {\"file_path\": \"/Users/Yuxuan_Zhang/Downloads/repo/rag_demo/../data/paul_graham/paul_graham_essay.txt\", \"file_name\": \"paul_graham_essay.txt\", \"file_type\": \"text/plain\", \"file_size\": 75042, \"creation_date\": \"2025-02-15\", \"last_modified_date\": \"2025-02-15\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"514f15da-e129-434e-87ed-07ce5d10a701\", \"node_type\": \"4\", \"metadata\": {\"file_path\": \"/Users/Yuxuan_Zhang/Downloads/repo/rag_demo/../data/paul_graham/paul_graham_essay.txt\", \"file_name\": \"paul_graham_essay.txt\", \"file_type\": \"text/plain\", \"file_size\": 75042, \"creation_date\": \"2025-02-15\", \"last_modified_date\": \"2025-02-15\"}, \"hash\": \"0c3c3f46cac874b495d944dfc4b920f6b68817dbbb1699ecc955d1fafb2bf87b\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"0691733e-e56c-4230-b5e5-43dcafd6e3c2\", \"node_type\": \"1\", \"metadata\": {\"file_path\": \"/Users/Yuxuan_Zhang/Downloads/repo/rag_demo/../data/paul_graham/paul_graham_essay.txt\", \"file_name\": \"paul_graham_essay.txt\", \"file_type\": \"text/plain\", \"file_size\": 75042, \"creation_date\": \"2025-02-15\", \"last_modified_date\": \"2025-02-15\"}, \"hash\": \"92a5239245346c570b7abf81e9b897760fc42c3d9cbe7db97411286cfb79e1a0\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"43f65582-bc5b-4ae0-8780-2c52212317f7\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"59f3bfeece03bebef255dfaa3b8a2605a508a94c506392e3df1252b9414a487e\", \"class_name\": \"RelatedNodeInfo\"}}, \"metadata_template\": \"{key}: {value}\", \"metadata_separator\": \"\\\\n\", \"text\": \"\", \"mimetype\": \"text/plain\", \"start_char_idx\": 6982, \"end_char_idx\": 11175, \"metadata_seperator\": \"\\\\n\", \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"class_name\": \"TextNode\"}', '_node_type': 'TextNode', 'author': 'Paul Graham', 'creation_date': '2025-02-15', 'doc_id': '514f15da-e129-434e-87ed-07ce5d10a701', 'document_id': '514f15da-e129-434e-87ed-07ce5d10a701', 'file_name': 'paul_graham_essay.txt', 'file_path': '/Users/Yuxuan_Zhang/Downloads/repo/rag_demo/../data/paul_graham/paul_graham_essay.txt', 'file_size': 75042, 'file_type': 'text/plain', 'last_modified_date': '2025-02-15', 'ref_doc_id': '514f15da-e129-434e-87ed-07ce5d10a701'}\n",
            "count before 64\n",
            "count after 63\n"
          ]
        }
      ],
      "source": [
        "doc_to_update = chroma_collection.get(limit=1)\n",
        "doc_to_update[\"metadatas\"][0] = {\n",
        "    **doc_to_update[\"metadatas\"][0],\n",
        "    **{\"author\": \"Paul Graham\"},\n",
        "}\n",
        "chroma_collection.update(\n",
        "    ids=[doc_to_update[\"ids\"][0]], metadatas=[doc_to_update[\"metadatas\"][0]]\n",
        ")\n",
        "updated_doc = chroma_collection.get(limit=1)\n",
        "print(updated_doc[\"metadatas\"][0])\n",
        "\n",
        "# delete the last document\n",
        "print(\"count before\", chroma_collection.count())\n",
        "chroma_collection.delete(ids=[doc_to_update[\"ids\"][0]])\n",
        "print(\"count after\", chroma_collection.count())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rag-demo-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
