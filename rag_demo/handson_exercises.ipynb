{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d97773c-1926-43ee-bfe2-0da4c53ec9da",
   "metadata": {},
   "source": [
    "# Day 1 - RAG\n",
    "# 1. Vector Embedding and Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e41d85f-a39c-4090-a25d-3742a280c618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rag-demo-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b05932-9e05-4a61-8a5b-d61299657324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model (runs locally)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e90a67-e96d-4db6-b9d8-989cb0c2684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"The Eiffel Tower is in Paris.\",\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"Python is a popular programming language.\",\n",
    "    \"Machine learning enables AI applications.\",\n",
    "    \"Sentiment analysis helps understand customer feedback.\",\n",
    "    \"Stock market predictions are complex and data-driven.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "831318f8-240f-48ce-b5cd-e0e4773c1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "embeddings = model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3570c87-9674-41e6-9e16-f6b796d5b8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings stored in FAISS index.\n"
     ]
    }
   ],
   "source": [
    "# Store embeddings in FAISS\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "print(\"Embeddings stored in FAISS index.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d120ce86-2cad-4143-9a66-79b49b654d98",
   "metadata": {},
   "source": [
    "# 2. Information Retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a671fe1-8f2e-480b-8d6e-68c055a74d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "# query = \"Where is the Eiffel Tower?\"\n",
    "query = \"What are some popular programming languages?\"\n",
    "query_embedding = model.encode([query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "139be242-4b9a-4cd1-926b-52279d076653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved document: Python is a popular programming language.\n"
     ]
    }
   ],
   "source": [
    "# Search top-1 nearest neighbor\n",
    "distances, indices = index.search(np.array(query_embedding), k=1)\n",
    "retrieved_doc = documents[indices[0][0]]\n",
    "\n",
    "print(\"Retrieved document:\", retrieved_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca3bfb2-f632-4aa4-b554-ad700c621cc5",
   "metadata": {},
   "source": [
    "# Day 2 - Scaling and Serving\n",
    "# 1. Large-scale Data Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533064f0-66b6-4d31-b129-ca739684d12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a local LLM (GPT4All, Llama2, etc.)\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\") # Using a sentiment analysis model to label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8482d02b-f409-430d-b53e-d6344d17ed79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: I am blown away! This product is so great!\n",
      "Sentiment: POSITIVE\n",
      "\n",
      "Document: If given a choice, I would never come back again\n",
      "Sentiment: NEGATIVE\n",
      "\n",
      "Document: It is fine. Some slow parts and some good moments in the show\n",
      "Sentiment: POSITIVE\n",
      "\n",
      "Document: Oh great, another softwware update that breaks everything!\n",
      "Sentiment: POSITIVE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "senti_documents = [\n",
    "    \"I am blown away! This product is so great!\",\n",
    "    \"If given a choice, I would never come back again\",\n",
    "    \"It is fine. Some slow parts and some good moments in the show\",\n",
    "    \"Oh great, another softwware update that breaks everything!\"\n",
    "]\n",
    "sentiments = []\n",
    "# Loop through each document and analyze sentiment\n",
    "for doc in senti_documents:\n",
    "    sentiment_result = sentiment_pipeline(doc)\n",
    "    # Append the result (label and score) to the sentiments list\n",
    "    sentiments.append(sentiment_result[0]['label'])\n",
    "\n",
    "# Output the sentiment for each document\n",
    "for doc, sentiment in zip(senti_documents, sentiments):\n",
    "    print(f\"Document: {doc}\\nSentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c505a7bd-4f09-479c-b15c-b1596fb10267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: The stock market surged today as investors responded positively to new economic policies.\n",
      "Predicted Topic: Business\n",
      "\n",
      "Document: The local football team has won the championship after a nail-biting final match.\n",
      "Predicted Topic: Sports\n",
      "\n",
      "Document: New advancements in artificial intelligence are revolutionizing industries worldwide.\n",
      "Predicted Topic: Technology\n",
      "\n",
      "Document: The government has introduced new policies aimed at addressing climate change.\n",
      "Predicted Topic: Politics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text classification pipeline (using a pre-trained news classification model)\n",
    "# We will use \"distilbert-base-uncased\" fine-tuned for topic classification or any other suitable model\n",
    "topic_classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# List of news articles\n",
    "news_documents = [\n",
    "    \"The stock market surged today as investors responded positively to new economic policies.\",\n",
    "    \"The local football team has won the championship after a nail-biting final match.\",\n",
    "    \"New advancements in artificial intelligence are revolutionizing industries worldwide.\",\n",
    "    \"The government has introduced new policies aimed at addressing climate change.\"\n",
    "]\n",
    "\n",
    "# Define possible categories (topics) for classification\n",
    "candidate_labels = [\"Politics\", \"Technology\", \"Sports\", \"Business\", \"Health\", \"Entertainment\"]\n",
    "\n",
    "# Store the predicted topics\n",
    "topics = []\n",
    "\n",
    "# Loop through each document and classify it\n",
    "for doc in news_documents:\n",
    "    topic_result = topic_classifier(doc, candidate_labels=candidate_labels)\n",
    "    # Append the predicted topic to the topics list\n",
    "    topics.append(topic_result['labels'][0])  # We use the highest probability label\n",
    "\n",
    "# Output the topic for each document\n",
    "for doc, topic in zip(news_documents, topics):\n",
    "    print(f\"Document: {doc}\\nPredicted Topic: {topic}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-demo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
